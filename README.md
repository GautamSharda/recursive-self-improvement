## Motivations

Can we build an agent that improves its own performance on the [ARC-AGI](https://arcprize.org/arc-agi/2/) benchmark? For example, consider any of the new coding agents (claude code, gemini cli, openai's codex, etc) -- what is their baseline performance? Can they improve this performance by modifying their own prompts or source code? By how much? What is the scaffolding they choose? 

Can we (humans) produce better "seed" scaffoldings (read: prompts, though could be more) that result in more improvement or better performance? If there is a limit, what is it? Why does it occur? And what is the appropriate monitoring infrastructure, scripts, and software to conduct such experiments?

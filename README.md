Motivations: Can we build an agent that improves its own performance on the arc-agi benchmark? For example, consider any of the new coding agents (claude code, gemini cli, openai's codex, etc) -- what is their baseline performance? can they improve this performance by modifying their own prompts or source code? by how much? what is the scaffolding they choose? can we (humans) produce  better "seed" scaffoldings (read: prompts, though could be more) that result in more improvement or better performance? if there is a limit, what is it? why does it occur? and what is the appropriate monitoring infrasutrcure to conduct such experiments?    
